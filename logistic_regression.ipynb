{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I imported all packages that are needed to be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from util import *\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I loaded the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = load_data(\"data/processed/dataset_1.csv\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I checked the first few instances of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>9.029</td>\n",
       "      <td>17.33</td>\n",
       "      <td>58.79</td>\n",
       "      <td>250.5</td>\n",
       "      <td>0.10660</td>\n",
       "      <td>0.14130</td>\n",
       "      <td>0.31300</td>\n",
       "      <td>0.04375</td>\n",
       "      <td>0.2111</td>\n",
       "      <td>0.08046</td>\n",
       "      <td>...</td>\n",
       "      <td>10.31</td>\n",
       "      <td>22.65</td>\n",
       "      <td>65.50</td>\n",
       "      <td>324.7</td>\n",
       "      <td>0.14820</td>\n",
       "      <td>0.43650</td>\n",
       "      <td>1.25200</td>\n",
       "      <td>0.17500</td>\n",
       "      <td>0.4228</td>\n",
       "      <td>0.11750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>21.090</td>\n",
       "      <td>26.57</td>\n",
       "      <td>142.70</td>\n",
       "      <td>1311.0</td>\n",
       "      <td>0.11410</td>\n",
       "      <td>0.28320</td>\n",
       "      <td>0.24870</td>\n",
       "      <td>0.14960</td>\n",
       "      <td>0.2395</td>\n",
       "      <td>0.07398</td>\n",
       "      <td>...</td>\n",
       "      <td>26.68</td>\n",
       "      <td>33.48</td>\n",
       "      <td>176.50</td>\n",
       "      <td>2089.0</td>\n",
       "      <td>0.14910</td>\n",
       "      <td>0.75840</td>\n",
       "      <td>0.67800</td>\n",
       "      <td>0.29030</td>\n",
       "      <td>0.4098</td>\n",
       "      <td>0.12840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>9.173</td>\n",
       "      <td>13.86</td>\n",
       "      <td>59.20</td>\n",
       "      <td>260.9</td>\n",
       "      <td>0.07721</td>\n",
       "      <td>0.08751</td>\n",
       "      <td>0.05988</td>\n",
       "      <td>0.02180</td>\n",
       "      <td>0.2341</td>\n",
       "      <td>0.06963</td>\n",
       "      <td>...</td>\n",
       "      <td>10.01</td>\n",
       "      <td>19.23</td>\n",
       "      <td>65.59</td>\n",
       "      <td>310.1</td>\n",
       "      <td>0.09836</td>\n",
       "      <td>0.16780</td>\n",
       "      <td>0.13970</td>\n",
       "      <td>0.05087</td>\n",
       "      <td>0.3282</td>\n",
       "      <td>0.08490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>10.650</td>\n",
       "      <td>25.22</td>\n",
       "      <td>68.01</td>\n",
       "      <td>347.0</td>\n",
       "      <td>0.09657</td>\n",
       "      <td>0.07234</td>\n",
       "      <td>0.02379</td>\n",
       "      <td>0.01615</td>\n",
       "      <td>0.1897</td>\n",
       "      <td>0.06329</td>\n",
       "      <td>...</td>\n",
       "      <td>12.25</td>\n",
       "      <td>35.19</td>\n",
       "      <td>77.98</td>\n",
       "      <td>455.7</td>\n",
       "      <td>0.14990</td>\n",
       "      <td>0.13980</td>\n",
       "      <td>0.11250</td>\n",
       "      <td>0.06136</td>\n",
       "      <td>0.3409</td>\n",
       "      <td>0.08147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>10.170</td>\n",
       "      <td>14.88</td>\n",
       "      <td>64.55</td>\n",
       "      <td>311.9</td>\n",
       "      <td>0.11340</td>\n",
       "      <td>0.08061</td>\n",
       "      <td>0.01084</td>\n",
       "      <td>0.01290</td>\n",
       "      <td>0.2743</td>\n",
       "      <td>0.06960</td>\n",
       "      <td>...</td>\n",
       "      <td>11.02</td>\n",
       "      <td>17.45</td>\n",
       "      <td>69.86</td>\n",
       "      <td>368.6</td>\n",
       "      <td>0.12750</td>\n",
       "      <td>0.09866</td>\n",
       "      <td>0.02168</td>\n",
       "      <td>0.02579</td>\n",
       "      <td>0.3557</td>\n",
       "      <td>0.08020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     radius_mean  texture_mean  perimeter_mean  area_mean  smoothness_mean  \\\n",
       "68         9.029         17.33           58.79      250.5          0.10660   \n",
       "181       21.090         26.57          142.70     1311.0          0.11410   \n",
       "63         9.173         13.86           59.20      260.9          0.07721   \n",
       "248       10.650         25.22           68.01      347.0          0.09657   \n",
       "60        10.170         14.88           64.55      311.9          0.11340   \n",
       "\n",
       "     compactness_mean  concavity_mean  concave points_mean  symmetry_mean  \\\n",
       "68            0.14130         0.31300              0.04375         0.2111   \n",
       "181           0.28320         0.24870              0.14960         0.2395   \n",
       "63            0.08751         0.05988              0.02180         0.2341   \n",
       "248           0.07234         0.02379              0.01615         0.1897   \n",
       "60            0.08061         0.01084              0.01290         0.2743   \n",
       "\n",
       "     fractal_dimension_mean  ...  radius_worst  texture_worst  \\\n",
       "68                  0.08046  ...         10.31          22.65   \n",
       "181                 0.07398  ...         26.68          33.48   \n",
       "63                  0.06963  ...         10.01          19.23   \n",
       "248                 0.06329  ...         12.25          35.19   \n",
       "60                  0.06960  ...         11.02          17.45   \n",
       "\n",
       "     perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "68             65.50       324.7           0.14820            0.43650   \n",
       "181           176.50      2089.0           0.14910            0.75840   \n",
       "63             65.59       310.1           0.09836            0.16780   \n",
       "248            77.98       455.7           0.14990            0.13980   \n",
       "60             69.86       368.6           0.12750            0.09866   \n",
       "\n",
       "     concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "68           1.25200               0.17500          0.4228   \n",
       "181          0.67800               0.29030          0.4098   \n",
       "63           0.13970               0.05087          0.3282   \n",
       "248          0.11250               0.06136          0.3409   \n",
       "60           0.02168               0.02579          0.3557   \n",
       "\n",
       "     fractal_dimension_worst  \n",
       "68                   0.11750  \n",
       "181                  0.12840  \n",
       "63                   0.08490  \n",
       "248                  0.08147  \n",
       "60                   0.08020  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "68     0\n",
       "181    1\n",
       "63     0\n",
       "248    0\n",
       "60     0\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, I checked the dimensions of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_train is:  (455, 30)\n",
      "The shape of y_train is:  (455,)\n"
     ]
    }
   ],
   "source": [
    "print(\"The shape of X_train is: \", X_train.shape)\n",
    "print(\"The shape of y_train is: \", y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I convert them to numpy for easier calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1. Sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In logistic regression, the model is presented as:\n",
    "$$ f_{\\mathbf{w},b}(x) = g(\\mathbf{w}\\cdot \\mathbf{x} + b)$$\n",
    "Where function $g$ is the sigmoid function. The formula for the sigmoid function is:\n",
    "$$g(z) = \\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, I implemented the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of z\n",
    "\n",
    "    Args:\n",
    "        z (ndarray): A numpy array.\n",
    "\n",
    "    Returns:\n",
    "        g (ndarray): sigmoid(z), with the same shape as z.\n",
    "         \n",
    "    \"\"\"\n",
    "    g = 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, I checked to see if the sigmoid function was implemented correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All test cases passed!\n"
     ]
    }
   ],
   "source": [
    "def test_sigmoid():\n",
    "    # Case 1: z is a scalar\n",
    "    z1 = 0\n",
    "    expected1 = 0.5\n",
    "    assert np.isclose(sigmoid(z1), expected1), f\"Expected {expected1}, but got {sigmoid(z1)}\"\n",
    "\n",
    "    # Case 2: z is a numpy array of shape (5,)\n",
    "    z2 = np.array([0, 2, -2, 1, -1])\n",
    "    expected2 = np.array([0.5, 0.88079708, 0.11920292, 0.73105858, 0.26894142])\n",
    "    assert np.allclose(sigmoid(z2), expected2), f\"Expected {expected2}, but got {sigmoid(z2)}\"\n",
    "\n",
    "    # Case 3: z is a matrix\n",
    "    z3 = np.array([[0, 2], [-2, 1]])\n",
    "    expected3 = np.array([[0.5, 0.88079708], [0.11920292, 0.73105858]])\n",
    "    assert np.allclose(sigmoid(z3), expected3), f\"Expected {expected3}, but got {sigmoid(z3)}\"\n",
    "\n",
    "    print(\"All test cases passed!\")\n",
    "\n",
    "test_sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the sigmoid function works properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2. Cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I implemented the cost function. For logistic regression, the cost function is of the form: \n",
    "$$ J(\\mathbf{w},b) = \\frac{1}{m}\\sum_{i=0}^{m-1} \\left[ loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) \\right]$$\n",
    "\n",
    "where\n",
    "* m is the number of training examples in the dataset\n",
    "\n",
    "\n",
    "* The cost for a single data point is calculated as:\n",
    "\n",
    "    $$loss(f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}), y^{(i)}) = (-y^{(i)} \\log\\left(f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right) - \\left( 1 - y^{(i)}\\right) \\log \\left( 1 - f_{\\mathbf{w},b}\\left( \\mathbf{x}^{(i)} \\right) \\right)$$\n",
    "    \n",
    "    \n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)})$ is the model's prediction\n",
    "\n",
    "*  $y^{(i)}$ is the actual label\n",
    "\n",
    "*  $f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) = g(\\mathbf{w} \\cdot \\mathbf{x^{(i)}} + b)$ where function $g$ is the sigmoid function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(X, y, w, b):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      y : (ndarray Shape (m,))  target value \n",
    "      w : (ndarray Shape (n,))  values of parameters of the model      \n",
    "      b : (scalar)              value of bias parameter of the model\n",
    "    Returns:\n",
    "      cost : (scalar) cost \n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # z = X.dot(w) + b\n",
    "    # g = sigmoid(z)\n",
    "    # loss = (-y*np.log(g)) - ((1-y)*np.log(1-g))\n",
    "    # cost = 1/m * np.sum(loss)\n",
    "    \n",
    "    cost = 0\n",
    "    for i in range(m):\n",
    "        z = np.dot(X[i],w) + b\n",
    "        f_wb = sigmoid(z)\n",
    "        cost += -y[i]*np.log(f_wb) - (1-y[i])*np.log(1-f_wb)\n",
    "    cost = cost/m\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost function test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_cost_function():\n",
    "    # Define test cases\n",
    "    X_test_case = np.array([[1, 2], [3, 4]])\n",
    "    y_test_case = np.array([0, 1])\n",
    "    w_test_case = np.array([0.1, 0.2])\n",
    "    b_test_case = 0.5\n",
    "\n",
    "    # Expected cost\n",
    "    expected_cost = 0.749\n",
    "\n",
    "    # Compute the cost using the cost_function\n",
    "    computed_cost = cost_function(X_test_case, y_test_case, w_test_case, b_test_case)\n",
    "\n",
    "    # Check if the computed cost is close to the expected cost\n",
    "    assert np.isclose(computed_cost, expected_cost, atol=0.01), f\"Expected {expected_cost}, but got {computed_cost}\"\n",
    "\n",
    "    print(\"Cost function test passed!\")\n",
    "\n",
    "test_cost_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the cost function works fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, I implemented the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pseudo code for gradient descent algorithm is:\n",
    "$$\\begin{align*}& \\text{repeat until convergence:} \\; \\lbrace \\newline \\; & b := b -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial b} \\newline       \\; & w_j := w_j -  \\alpha \\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j} \\tag{1}  \\; & \\text{for j := 0..n-1}\\newline & \\rbrace\\end{align*}$$\n",
    "\n",
    "where, parameters $b$ and $w_j$ are all updated simultaniously"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For logistic regression, the gradients are calculated as:\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial b}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)}) \\tag{2}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial J(\\mathbf{w},b)}{\\partial w_j}  = \\frac{1}{m} \\sum\\limits_{i = 0}^{m-1} (f_{\\mathbf{w},b}(\\mathbf{x}^{(i)}) - \\mathbf{y}^{(i)})x_{j}^{(i)} \\tag{3}\n",
    "$$\n",
    "* m is the number of training examples in the dataset.\n",
    "*  $f_{\\mathbf{w},b}(x^{(i)})$ is the model's prediction.\n",
    "*  $y^{(i)}$ is the actual label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I created a function for calculating the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(X, y, w, b, lambda_=None):\n",
    "    \"\"\"\n",
    "    Computes the gradient for logistic regression \n",
    " \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      y : (ndarray Shape (m,))  target value \n",
    "      w : (ndarray Shape (n,))  values of parameters of the model      \n",
    "      b : (scalar)              value of bias parameter of the model\n",
    "    Returns\n",
    "      dj_dw : (ndarray Shape (n,)) The gradient of the cost with respect to the parameters w. \n",
    "      dj_db : (scalar)             The gradient of the cost with respect to the parameter b. \n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    dj_dw = np.zeros(w.shape)\n",
    "    \n",
    "    z_wb = X.dot(w) + b\n",
    "    f_wb = sigmoid(z_wb)\n",
    "\n",
    "    dj_db = np.sum(f_wb - y) / m\n",
    "\n",
    "    for i in range(n):\n",
    "        dj_dw[i] = np.sum(np.dot((f_wb - y), (X.T)[i])) / m\n",
    "   \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compute gradient test passed!\n"
     ]
    }
   ],
   "source": [
    "def test_compute_gradient():\n",
    "    # Define test cases\n",
    "    X_test_case = np.array([[1, 2], [3, 4]])\n",
    "    y_test_case = np.array([0, 1])\n",
    "    w_test_case = np.array([0.1, 0.2])\n",
    "    b_test_case = 0.5\n",
    "\n",
    "    # Expected gradients\n",
    "    expected_dj_db = 0.2816\n",
    "    expected_dj_dw = np.array([0.1135, 0.3951])\n",
    "\n",
    "    # Compute the gradients using the compute_gradient function\n",
    "    computed_dj_db, computed_dj_dw = compute_gradient(X_test_case, y_test_case, w_test_case, b_test_case)\n",
    "\n",
    "    # Check if the computed gradients are close to the expected gradients\n",
    "    assert np.isclose(computed_dj_db, expected_dj_db, atol=0.01), f\"Expected dj_db {expected_dj_db}, but got {computed_dj_db}\"\n",
    "    assert np.allclose(computed_dj_dw, expected_dj_dw, atol=0.01), f\"Expected dj_dw {expected_dj_dw}, but got {computed_dj_dw}\"\n",
    "\n",
    "    print(\"Compute gradient test passed!\")\n",
    "\n",
    "test_compute_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function for computing gradient works just fine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I created a function to perform the gradient descent algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_): \n",
    "    \"\"\"\n",
    "    Performs batch gradient descent to learn theta. Updates theta by taking \n",
    "    num_iters gradient steps with learning rate alpha\n",
    "    \n",
    "    Args:\n",
    "      X :    (ndarray Shape (m, n) data, m examples by n features\n",
    "      y :    (ndarray Shape (m,))  target value \n",
    "      w_in : (ndarray Shape (n,))  Initial values of parameters of the model\n",
    "      b_in : (scalar)              Initial value of parameter of the model\n",
    "      cost_function :              function to compute cost\n",
    "      gradient_function :          function to compute gradient\n",
    "      alpha : (float)              Learning rate\n",
    "      num_iters : (int)            number of iterations to run gradient descent\n",
    "      lambda_ : (scalar, float)    regularization constant\n",
    "      \n",
    "    Returns:\n",
    "      w : (ndarray Shape (n,)) Updated values of parameters of the model after\n",
    "          running gradient descent\n",
    "      b : (scalar)                Updated value of parameter of the model after\n",
    "          running gradient descent\n",
    "    \"\"\"\n",
    "    \n",
    "    # number of training examples\n",
    "    m = len(X)\n",
    "    \n",
    "    # An array to store cost J and w's at each iteration primarily for graphing later\n",
    "    J_history = []\n",
    "    w_history = []\n",
    "    \n",
    "    for i in range(num_iters):\n",
    "\n",
    "        # Calculate the gradient and update the parameters\n",
    "        dj_db, dj_dw = gradient_function(X, y, w_in, b_in, lambda_)   \n",
    "\n",
    "        # Update Parameters using w, b, alpha and gradient\n",
    "        w_in = w_in - alpha * dj_dw               \n",
    "        b_in = b_in - alpha * dj_db              \n",
    "       \n",
    "        # Save cost J at each iteration\n",
    "        if i<100000:      # prevent resource exhaustion \n",
    "            cost =  cost_function(X, y, w_in, b_in, lambda_)\n",
    "            J_history.append(cost)\n",
    "\n",
    "        # Print cost every at intervals 10 times or as many iterations if < 10\n",
    "        if i% math.ceil(num_iters/10) == 0 or i == (num_iters-1):\n",
    "            w_history.append(w_in)\n",
    "            print(f\"Iteration {i:4}: Cost {float(J_history[-1]):8.2f}   \")\n",
    "        \n",
    "    return w_in, b_in, J_history, w_history #return w and J,w history for graphing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cost_function() takes 4 positional arguments but 5 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m iterations \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[0;32m      7\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.001\u001b[39m\n\u001b[1;32m----> 9\u001b[0m w,b, J_history,_ \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_w\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial_b\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                                   \u001b[49m\u001b[43mcost_function\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompute_gradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 42\u001b[0m, in \u001b[0;36mgradient_descent\u001b[1;34m(X, y, w_in, b_in, cost_function, gradient_function, alpha, num_iters, lambda_)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Save cost J at each iteration\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m i\u001b[38;5;241m<\u001b[39m\u001b[38;5;241m100000\u001b[39m:      \u001b[38;5;66;03m# prevent resource exhaustion \u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     cost \u001b[38;5;241m=\u001b[39m  \u001b[43mcost_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m     J_history\u001b[38;5;241m.\u001b[39mappend(cost)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Print cost every at intervals 10 times or as many iterations if < 10\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: cost_function() takes 4 positional arguments but 5 were given"
     ]
    }
   ],
   "source": [
    "np.random.seed(1)\n",
    "initial_w = 0.01 * (np.random.rand(30) - 0.5)\n",
    "initial_b = -8\n",
    "\n",
    "# Some gradient descent settings\n",
    "iterations = 1000\n",
    "alpha = 0.001\n",
    "\n",
    "w,b, J_history,_ = gradient_descent(X_train ,y_train, initial_w, initial_b, \n",
    "                                   cost_function, compute_gradient, alpha, iterations, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X, w, b): \n",
    "    \"\"\"\n",
    "    Predict whether the label is 0 or 1 using learned logistic\n",
    "    regression parameters w\n",
    "    \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      w : (ndarray Shape (n,))  values of parameters of the model      \n",
    "      b : (scalar)              value of bias parameter of the model\n",
    "\n",
    "    Returns:\n",
    "      p : (ndarray (m,)) The predictions for X using a threshold at 0.5\n",
    "    \"\"\"\n",
    "    # number of training examples\n",
    "    m, n = X.shape   \n",
    "    p = np.zeros(m)\n",
    "    \n",
    "    # Loop over each example\n",
    "    for i in range(m):   \n",
    "        z_wb = np.dot(w, X.T) + b\n",
    "        \n",
    "        # Calculate the prediction for this example\n",
    "        f_wb = sigmoid(z_wb)\n",
    "\n",
    "        # Apply the threshold\n",
    "        p[i] = 1 if f_wb[i] >= 0.5 else 0\n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_reg(X, y, w, b, lambda_ = 1):\n",
    "    \"\"\"\n",
    "    Computes the cost over all examples\n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      y : (ndarray Shape (m,))  target value \n",
    "      w : (ndarray Shape (n,))  values of parameters of the model      \n",
    "      b : (scalar)              value of bias parameter of the model\n",
    "      lambda_ : (scalar, float) Controls amount of regularization\n",
    "    Returns:\n",
    "      total_cost : (scalar)     cost \n",
    "    \"\"\"\n",
    "\n",
    "    m, n = X.shape\n",
    "    \n",
    "    # Calls the compute_cost function that you implemented above\n",
    "    cost_without_reg = cost_function(X, y, w, b) \n",
    "    \n",
    "    # You need to calculate this value\n",
    "    reg_cost = 0.\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "    reg_cost = lambda_ / (2*m) * np.sum(np.square(w))\n",
    "        \n",
    "    \n",
    "    ### END CODE HERE ### \n",
    "    \n",
    "    # Add the regularization cost to get the total cost\n",
    "    total_cost = cost_without_reg + reg_cost\n",
    "\n",
    "    return total_cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient_reg(X, y, w, b, lambda_ = 1): \n",
    "    \"\"\"\n",
    "    Computes the gradient for logistic regression with regularization\n",
    " \n",
    "    Args:\n",
    "      X : (ndarray Shape (m,n)) data, m examples by n features\n",
    "      y : (ndarray Shape (m,))  target value \n",
    "      w : (ndarray Shape (n,))  values of parameters of the model      \n",
    "      b : (scalar)              value of bias parameter of the model\n",
    "      lambda_ : (scalar,float)  regularization constant\n",
    "    Returns\n",
    "      dj_db : (scalar)             The gradient of the cost w.r.t. the parameter b. \n",
    "      dj_dw : (ndarray Shape (n,)) The gradient of the cost w.r.t. the parameters w. \n",
    "\n",
    "    \"\"\"\n",
    "    m, n = X.shape\n",
    "    \n",
    "    dj_db, dj_dw = compute_gradient(X, y, w, b)\n",
    "\n",
    "    ### START CODE HERE ###     \n",
    "    regularization = lambda_ / m * w\n",
    "    dj_dw = dj_dw + regularization\n",
    "        \n",
    "    ### END CODE HERE ###         \n",
    "        \n",
    "    return dj_db, dj_dw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.956140350877193"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create an instance of LogisticRegression\n",
    "log_reg = LogisticRegression()\n",
    "\n",
    "# Fit the model on the training data\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "log_reg.score(X_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "y_pred = log_reg.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
